# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yt2lZ973jzHWxPaxrCNEQ9KV2H9g4kcq
"""



import numpy as np
import librosa
import os

# Function to extract pitch and volume from a local audio file
def extract_pitch_and_volume(audio_path):
    # Ensure the file exists
    if not os.path.exists(audio_path):
        print("Audio file not found.")
        return None, None

    # Load audio file
    y, sr = librosa.load(audio_path)

    # Extract pitch using pyin
    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
    f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))

    # Get the mean pitch value for the voiced parts
    f0 = f0[voiced_flag]  # Filter only voiced frames
    average_pitch = np.mean(f0) if f0.size > 0 else 0

    # Calculate RMS volume
    volume = librosa.feature.rms(y=y)
    average_volume = np.mean(volume) if volume.size > 0 else 0  # Ensure volume is not empty

    return average_pitch, average_volume

# Provide the path to your local audio file
audio_path = "/Users/rhychaw/hackathon/Rhythm/Models/harvard.wav"  # Replace this with your actual local file path

# Extract pitch and volume
average_pitch, average_volume = extract_pitch_and_volume(audio_path)

if average_pitch is not None:
    print(f"Average Pitch: {average_pitch:.2f} Hz")
    print(f"Average Volume (RMS): {average_volume:.2f}")



from openai import OpenAI

client = OpenAI(
  api_key='sk-oWYM6CsmDX9lJv0seNROpPHKAtQTq5atImqKgTCHODT3BlbkFJHbXZB4dE2L_hX5r1uKGp3AFpZg2rcW7B6rN5IuXH0A'
)
# Function to get emphasis points from ChatGPT
def get_emphasis_points(transcript):
    # Create the prompt for emphasis points
    prompt = f"""
    I have a part of speech. I want you to extract words or sometines phrases of 2 to 3 words
    where the speaker must bring a change in either their tone, pitch or volume to
    keep the audience engaged. Return the words or phrases in a comma separated list.

    Here's the speech:
    "{transcript}"
    """

    # Use the chat completions endpoint with the prompt
    response = client.chat.completions.create( # Changed from client.completions.create
        model="gpt-3.5-turbo",
        messages=[{"role": "user", "content": prompt}] # Added messages parameter for chat models
    )

    # Return the suggestions
    return response.choices[0].message.content.strip() # Access the content from the message

# Example usage
transcript = """So, 2014 was a big year for me.
Do you ever have that, just like a big year,
like a banner year? For me, it went like this: October 3,
I lost my second pregnancy. And then October 8, my dad died of cancer.
And then on November 25, my husband Aaron died after three years with
stage-four glioblastoma, which is just a fancy word for brain cancer."""

emphasis_points = get_emphasis_points(transcript)
print("Emphasis Points: ", emphasis_points)

with open('/Users/rhychaw/hackathon/Rhythm/Models/results/emphasis.txt', 'w') as file:
    file.write(emphasis_points)

import speech_recognition as sr
from pydub import AudioSegment
import tempfile

# Load your audio file
audio_file = "/Users/rhychaw/hackathon/Rhythm/Models/harvard.wav"
audio = AudioSegment.from_wav(audio_file)

# Initialize recognizer
recognizer = sr.Recognizer()

# Convert audio to chunks and recognize each chunk
words_with_timestamps = []
chunk_length_ms = 2000  # Use 2 seconds per chunk

for start_time in range(0, len(audio), chunk_length_ms):
    chunk = audio[start_time:start_time + chunk_length_ms]

    # Save the chunk to a temporary file
    with tempfile.NamedTemporaryFile(delete=True, suffix='.wav') as temp_file:
        chunk.export(temp_file.name, format="wav")

        with sr.AudioFile(temp_file.name) as source:
            audio_data = recognizer.record(source)
            try:
                print(f"Processing chunk from {start_time / 1000:.2f}s")
                result = recognizer.recognize_google(audio_data, show_all=True)

                # Check if any results are returned
                if result and isinstance(result, list) and len(result) > 0:
                    alternative = result[0]
                    if 'transcript' in alternative:
                        for word in alternative['transcript'].split():
                            end_time = start_time + chunk_length_ms
                            words_with_timestamps.append({
                                'word': word,
                                'start': start_time / 1000.0,  # Convert to seconds
                                'end': end_time / 1000.0
                            })
                    else:
                        print("No transcript found in alternative.")
                else:
                    print("No results returned from Google Speech Recognition.")
            except sr.UnknownValueError:
                print("Could not understand audio")
            except sr.RequestError as e:
                print(f"Could not request results from Google Speech Recognition service; {e}")

# Print the words with their timestamps
for word_info in words_with_timestamps:
    print(f"Word: {word_info['word']}, Start: {word_info['start']:.2f}s, End: {word_info['end']:.2f}s")
    