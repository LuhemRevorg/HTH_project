# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r5pLu2HgJdEpQDQMnqli-Ar1-dBQoUdB
"""


import os
from google.cloud import speech
import io
import wave  # Import the wave module
from pydub import AudioSegment # Import AudioSegment for audio manipulation
import speech_recognition as sr
import librosa
import numpy as np
import speech_recognition as sr
from pydub import AudioSegment
from pydub.playback import play
from google.protobuf.duration_pb2 import Duration

import csv




os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "Models/vocal-operand-437123-u7-0ff753e4439f.json"



def main_ts():
    def get_word_timestamps(audio_file):
        recognizer = sr.Recognizer()

        # Load audio file
        with sr.AudioFile(audio_file) as source:
            audio = recognizer.record(source)

        # Use the Google Web Speech API to get transcription with timestamps
        try:
            # Note: This will return a dict with details including timestamps
            response = recognizer.recognize_google(audio, show_all=True)

            word_timestamps = {}

            # Check if response has alternatives and extract timestamps
            if response and 'alternative' in response:
                for alternative in response['alternative']:
                    if 'timestamps' in alternative:
                        for word_info in alternative['timestamps']:
                            word = word_info[0]
                            start_time = word_info[1]
                            end_time = word_info[2]
                            word_timestamps[word] = (start_time, end_time)

            return word_timestamps
        except sr.UnknownValueError:
            print("Speech Recognition could not understand audio")
            return {}
        except sr.RequestError as e:
            print(f"Could not request results from Google Speech Recognition service; {e}")
            return {}

    # Example usage:
    audio_file = '/Users/rhychaw/hackathon/Rhythm/backend/uploads/recording.wav'
    words_with_times = get_word_timestamps(audio_file)
    #print(words_with_times)




    # Function to extract pitch and volume from a specific segment
    def extract_pitch_and_volume(audio_segment, sr):
        if len(audio_segment) == 0:
            return 0, 0  # Return zeros if segment is empty

        # Extract pitch using pyin
        pitches, magnitudes = librosa.piptrack(y=audio_segment, sr=sr)
        f0, voiced_flag, _ = librosa.pyin(audio_segment, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))

        # Filter only voiced frames
        f0 = f0[voiced_flag]
        average_pitch = np.mean(f0) if f0.size > 0 else 0

        # Calculate RMS volume
        volume = librosa.feature.rms(y=audio_segment)
        average_volume = np.mean(volume) if volume.size > 0 else 0  # Ensure volume is not empty

        return average_pitch, average_volume

    # Function to get the segment of a specific word based on timestamps
    def get_word_timings(audio_file):
        recognizer = sr.Recognizer()
        with sr.AudioFile(audio_file) as source:
            audio_data = recognizer.record(source)

        try:
            # Recognize speech with Google API and request word timestamps
            result = recognizer.recognize_google(audio_data, show_all=True)

            if not result or 'alternative' not in result:
                raise Exception("No transcriptions found")

            alternatives = result['alternative']

            # Check if there is a timestamp in the response
            if len(alternatives) > 0 and 'timestamps' in alternatives[0]:
                return alternatives[0]['timestamps']
            else:
                print("Word timestamps not available in the API response.")
                return []

        except sr.UnknownValueError:
            print("Could not understand the audio")
            return []
        except sr.RequestError as e:
            print(f"Error with speech recognition service: {e}")
            return []

    # Function to analyze the pitch and volume for specific words in the audio
    def analyze_pitch_of_words(audio_file, word_list):
        # Load the audio file
        audio = AudioSegment.from_file(audio_file)

        # Get timings for words in the audio
        word_timings = get_word_timings(audio_file)

        # Iterate through the word list and find matches in the audio
        for word_data in word_timings:
            word = word_data[0]  # Word
            start_time = word_data[1] * 1000  # Convert to milliseconds
            end_time = word_data[2] * 1000  # Convert to milliseconds

            if word in word_list:
                # Extract the audio segment corresponding to the word
                word_audio_segment = audio[start_time:end_time]

                # Convert the pydub audio segment to numpy array for librosa
                samples = np.array(word_audio_segment.get_array_of_samples())
                sr = word_audio_segment.frame_rate

                # Get the pitch and volume for the word
                pitch, volume = extract_pitch_and_volume(samples, sr)

                #print(f"Word: {word}, Pitch: {pitch}, Volume: {volume}")

    # Usage
    audio_file = "/Users/rhychaw/hackathon/Rhythm/backend/uploads/recording.wav"  # Path to your audio file
    word_list = ["example", "words"]  # List of words to analyze

    analyze_pitch_of_words(audio_file, word_list)





    # Initialize Google Cloud Speech client
    client = speech.SpeechClient()

    def transcribe_audio_with_timestamps(audio_file):
        # Get the actual sample rate from the WAV file header
        with wave.open(audio_file, 'rb') as wf:
            sample_rate_hertz = wf.getframerate()
            num_channels = wf.getnchannels() # Get number of audio channels

        # Convert stereo audio to mono using pydub if num_channels > 1
        if num_channels > 1:
            sound = AudioSegment.from_wav(audio_file)
            sound = sound.set_channels(1)  # Convert to mono
            sound.export(audio_file, format="wav") # Overwrite original with mono audio

        with io.open(audio_file, "rb") as audio_file:
            content = audio_file.read()

        audio = speech.RecognitionAudio(content=content)

        config = speech.RecognitionConfig(
            encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,  # Adjust encoding if necessary
            sample_rate_hertz=sample_rate_hertz,  # Use the actual sample rate
            language_code="en-US",  # Language of the audio
            enable_word_time_offsets=True  # Enable word timestamps
        )

        # Transcribe audio
        response = client.recognize(config=config, audio=audio)

        words_with_timestamps = []

        for result in response.results:
            alternative = result.alternatives[0]
            print(f"Transcript: {alternative.transcript}")

            for word_info in alternative.words:
                word = word_info.word
                start_time = word_info.start_time
                end_time = word_info.end_time

                # Assuming start_time and end_time are Duration objects
                start_time_seconds = start_time.seconds + start_time.nanos / 1_000_000_000
                end_time_seconds = end_time.seconds + end_time.nanos / 1_000_000_000

                words_with_timestamps.append((word, start_time_seconds, end_time_seconds))

        return words_with_timestamps

    # Usage
    audio_file = "/Users/rhychaw/hackathon/Rhythm/backend/uploads/recording.wav"  # Path to the audio file
    timestamps = transcribe_audio_with_timestamps(audio_file)


    '''def extract_pitch_and_volume(audio_path):
        # Ensure the file exists
        if not os.path.exists(audio_path):
            print("Audio file not found.")
            return None, None

        # Load audio file
        y, sr = librosa.load(audio_path)

        # Extract pitch using pyin
        pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
        f0, voiced_flag, voiced_probs = librosa.pyin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))

        # Get the mean pitch value for the voiced parts
        f0 = f0[voiced_flag]  # Filter only voiced frames
        average_pitch = np.mean(f0) if f0.size > 0 else 0

        # Calculate RMS volume
        volume = librosa.feature.rms(y=y)
        average_volume = np.mean(volume) if volume.size > 0 else 0  # Ensure volume is not empty

        return average_pitch'''

    def extract_average_pitch(audio_path, start_time, end_time):
        # Ensure the file exists
        if not os.path.exists(audio_path):
            print("Audio file not found.")
            return None

        # Load the entire audio file
        y, sr = librosa.load(audio_path, sr=None)

        # Convert start_time and end_time from seconds to samples
        start_sample = int(start_time * sr)
        end_sample = int(end_time * sr)

        # Extract the audio segment for the specified duration
        audio_segment = y[start_sample:end_sample]

        # Extract pitch using pyin
        f0, voiced_flag, _ = librosa.pyin(audio_segment, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))

        # Get the mean pitch value for the voiced parts
        f0 = f0[voiced_flag]  # Filter only voiced frames
        average_pitch = np.mean(f0) if f0.size > 0 else 0  # Compute the average pitch

        return average_pitch

    # ... (rest of the code remains the same)

    def analyze_pitch_of_emphasized_words(audio_file, emphasized_words, word_timestamps):
        """
        Analyzes the pitch of emphasized words in an audio file using their timestamps.

        Args:
            audio_file (str): Path to the audio file.
            emphasized_words (list): A list of words to analyze for pitch.
            word_timestamps (list): A list of tuples containing (word, start_time, end_time) 
                                    for each word in the audio.

        Returns:
            dict: A dictionary mapping emphasized words to their average pitch.
        """
        
        audio = AudioSegment.from_file(audio_file)
        emphasized_word_pitches = {}

        for word, start_time, end_time in word_timestamps:
            if word in emphasized_words:
                # Extract the audio segment for the emphasized word
                word_audio_segment = audio[start_time * 1000:end_time * 1000] 
                
                # Convert to numpy array for librosa
                samples = np.array(word_audio_segment.get_array_of_samples())
                sr = word_audio_segment.frame_rate

                # Extract the pitch
                pitch = extract_average_pitch(audio_file, start_time, end_time)
                emphasized_word_pitches[word] = pitch

        return emphasized_word_pitches

    # Example usage
    audio_file = "/Users/rhychaw/hackathon/Rhythm/backend/uploads/recording.wav"
    '''emphasized_words = ['the', 'stale', 'smell', 'of', 'old', 'beer', 'lingers', 'it', 'takes', 'heat', 'to', 
                        'bring', 'out', 'the', 'odor', 'a', 'cold', 'dip', 'restores', 'health', 'and', 'zest', 'a', 'salt', 'pickle', 'tastes', 'fine' 
                        ,'with', 'ham', 'tacos', 'al', 'pastor', 'are', 'my', 'favorite', 'a', 'zestful', 'food',
                        'is', 'the', 'hot', 'cross', 'bun']''' 
    # Assuming 'timestamps' is the output from the previous Google Cloud Speech-to-Text code
    # timestamps = transcribe_audio_with_timestamps(audio_file) 

    # For testing purposes, let's create a sample timestamps list:
    #timestamps = [("this", 0.0, 0.5), ("is", 0.5, 0.8), ("an", 0.8, 1.0), ("example", 1.0, 1.5), ("of", 1.5, 1.7), ("some", 1.7, 2.0), ("words", 2.0, 2.5)]
    emphasized_words=[]
    for word in timestamps:
        emphasized_words.append(word[0])

    emphasized_word_pitches = analyze_pitch_of_emphasized_words(audio_file, emphasized_words, timestamps)

    keys_to_remove = []

    for word, pitch in emphasized_word_pitches.items():
        mean_pitch = np.mean(pitch)
        if np.isnan(mean_pitch) or mean_pitch==0:
            keys_to_remove.append(word)

    for key in keys_to_remove:
        emphasized_word_pitches.pop(key)

    '''for word, pitch in emphasized_word_pitches.items():
    mean_pitch = np.mean(pitch)
    print(f"Word: {word}, Mean Pitch: {mean_pitch}")'''

    #print(emphasized_word_pitches)
    #print(timestamps)


    common_entries = []
    seen_words = set()  # To track the words that have already been added

    for word in emphasized_word_pitches.keys():
        if word not in seen_words:  # Check if the word is already added
            for entry in timestamps:
                if entry[0] == word:
                    common_entries.append((word, emphasized_word_pitches[word], entry[1], entry[2]))
                    seen_words.add(word)  # Add the word to the set after adding the first instance
                    break  # Exit inner loop after adding the first instance

    # Save to CSV


    csv_filename = '/Users/rhychaw/hackathon/Rhythm/backend/Models/results/common_entries.csv'
    with open(csv_filename, mode='w', newline='') as file:
        writer = csv.writer(file)
        # Write header
        writer.writerow(['Word', 'DataDict Value', 'Start', 'End'])
        # Write data
        writer.writerows(common_entries)

# import sys
# sys.modules[__name__] = main_ts